# -*- coding: utf-8 -*-
"""tripto_fine_tuning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1quLkkgu4QD2mmxo9kK1My321prKtXDXa
"""


import transformers

import tensorflow as tf


from transformers import TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments

import numpy as np
import sentencepiece

from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
import torch

import pandas as pd
from sklearn.model_selection import train_test_split

"""=================

"""

# Load the pre-trained DistilBERT-base tokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

data = pd.read_csv('data.csv')

data.head()

data.isna().sum()

data = data.sample(frac=1)
data['encoded_labels'] = data['Label'].astype('category').cat.codes
data_texts = data["Question"].to_list() # Features (not-tokenized yet)
data_labels = data["encoded_labels"].to_list() # Lables

data['Label'].value_counts()

train_texts, val_texts, train_labels, val_labels = train_test_split(data_texts, data_labels, test_size=0.3, random_state=42, shuffle = True)

train_encodings = tokenizer(train_texts, truncation=True, padding=True,max_length=512)
val_encodings = tokenizer(val_texts, truncation=True, padding=True,max_length=512)

train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    train_labels
))
val_dataset = tf.data.Dataset.from_tensor_slices((
    dict(val_encodings),
    val_labels
))

training_args = TFTrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=200,             # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=32,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.1,               # strength of weight decay
    logging_dir='./logs',
    learning_rate=1e-4,              # Lower learning rate
    eval_steps=5,# directory for storing logs
)

with training_args.strategy.scope():
    trainer_model = TFDistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=10)

label_smoothing = 0.1
loss_fn = tf.keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing)
trainer_model.compile(loss=loss_fn, metrics=['accuracy'])

trainer = TFTrainer(
    model=trainer_model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

trainer.dropout = tf.keras.layers.Dropout(0.1)  # Add 0.1 dropout

trainer.train()

trainer.evaluate()

trainer_model.save_pretrained('/content/saved_models')
tokenizer.save_pretrained('/content/saved_models')

loaded_tokenizer = DistilBertTokenizer.from_pretrained('/content/saved_models')
loaded_model = TFDistilBertForSequenceClassification.from_pretrained('/content/saved_models')

test_text='r'
predict_input = loaded_tokenizer.encode(test_text,
                                 truncation=True,
                                 padding=True,
                                 return_tensors="tf")

output = loaded_model(predict_input)[0]

prediction_value = tf.argmax(output, axis=1).numpy()[0]
prediction_value

rows_with_4 = data[data['encoded_labels'] == 7]

print(rows_with_4)

"""
0  activity
1  category
2  description
3  goodbye
4  greetings
5  location
6  nearby
7  price
8  recommendattion
9  thanks
"""

# Find the label that corresponds to a specific encoded value
encoded_value = 7
corresponding_label = data['Label'].astype('category').cat.categories[encoded_value]

print(corresponding_label)